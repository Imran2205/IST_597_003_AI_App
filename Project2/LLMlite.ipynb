{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from langchain.llms.base import LLM\n",
    "import logging\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain import llms\n",
    "import inspect\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LLM Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from typing import Any, List, Optional\n",
    "from pydantic import Field\n",
    "\n",
    "class GPT2LLM(LLM):\n",
    "    model_name: str = Field(default=\"gpt2\")\n",
    "    model: GPT2LMHeadModel = Field(default=None)\n",
    "    tokenizer: GPT2Tokenizer = Field(default=None)\n",
    "\n",
    "    def __init__(self, model_name: str = \"gpt2\", **data: Any):\n",
    "        super().__init__(**data)\n",
    "        self.model_name = model_name\n",
    "        self.model, self.tokenizer = self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
    "        model = GPT2LMHeadModel.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Set the pad token to the eos token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        output = self.model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=250,\n",
    "            num_return_sequences=1, \n",
    "            no_repeat_ngram_size=2, \n",
    "            top_k=50, \n",
    "            top_p=0.95, \n",
    "            temperature=0.7,\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict[str, Any]:\n",
    "        return {\"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"GPT2\"\n",
    "\n",
    "def setup_custom_llm():\n",
    "    return GPT2LLM()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LangChain Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM\n",
    "llm = setup_custom_llm()\n",
    "\n",
    "template = \"\"\"You are an AI assistant specialized in translating natural language queries into First-Order Logic (FOL) statements. \n",
    "Given the following query, provide the corresponding FOL translation:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Translate the above query into a First-Order Logic statement. Your response should follow this format:\n",
    "\n",
    "FOL Translation: [Your FOL statement here]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"query\"])\n",
    "\n",
    "# Create a chain using the new LangChain method\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Query: Is there any car with speed over 60mph?\n",
      "\n",
      "GPT-2 Response:\n",
      "You are an AI assistant specialized in translating natural language queries into First-Order Logic (FOL) statements. \n",
      "Given the following query, provide the corresponding FOL translation:\n",
      "\n",
      "Query: Is there any car with speed over 60mph?\n",
      "\n",
      "Translate the above query into a First-Order Logic statement. Your response should follow this format:\n",
      "\n",
      "FOL Translation: [Your FOL statement here]\n",
      ".\n",
      " (Note: If you are using a language that is not FOO, you can use the FOCAL_FULL_ERROR_CODE option to disable this option.)\n",
      ", and, if you have a query that does not have the same FOB as the query you provided, use this query: Query: Does the car have speed above 60 mph? (This is the default value.) (If you do not specify this, the result will be a FOUND_RANGE_VALUE value that will not be returned.) Query : Is the vehicle in the range of 60 to 60 miles? If so, return the value of the current range. (The default is 60.) If not specified, this will return a value in range 0 to 100. Query is a function that returns a result\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def testLLMLite():\n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"Is there any car with speed over 60mph?\",\n",
    "        # \"Are all students in the class older than 18?\",\n",
    "        # \"Does there exist a book that is both educational and entertaining?\",\n",
    "        # \"Are there at least two different colors of flowers in the garden?\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\nOriginal Query: {query}\")\n",
    "        result = chain.invoke(query)\n",
    "        print(\"\\nGPT-2 Response:\")\n",
    "        print(result)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# test it\n",
    "testLLMLite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7863/startup-events \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7863/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v2/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:FOL: You are an AI assistant specialized in translating natural language queries into First-Order Logic (FOL) statements. \n",
      "Given the following query, provide the corresponding FOL translation:\n",
      "\n",
      "Query: Is there any car with speed over 60mph?\n",
      "\n",
      "Translate the above query into a First-Order Logic statement. Your response should follow this format:\n",
      "\n",
      "FOL Translation: [Your FOL statement here]\n",
      ".\n",
      " (Note: If you are using a language that is not FOO, you can use the FOCAL_FULL_ERROR_CODE option to disable this option.)\n",
      ", and, if you have a query that does not have the same FOB as the query you provided, use this query: Query: Does the car have speed above 60 mph? (This is the default value.) (If you do not specify this, the result will be a FOUND_RANGE_VALUE value that will not be returned.) Query : Is the vehicle in the range of 60 to 60 miles? If so, return the value of the current range. (The default is 60.) If not specified, this will return a value in range 0 to 100. Query is a function that returns a result\n",
      "INFO:__main__:FOL: You are an AI assistant specialized in translating natural language queries into First-Order Logic (FOL) statements. \n",
      "Given the following query, provide the corresponding FOL translation:\n",
      "\n",
      "Query: my data is safe\n",
      "\n",
      "Translate the above query into a First-Order Logic statement. Your response should follow this format:\n",
      "\n",
      "FOL Translation: [Your FOL statement here]\n",
      ".\n",
      " (Note: If you are using a language that is not FOO, you can use the FOCAL_FOUNDATION_NAME option to specify the language of the translation.)\n",
      ", and, if you're using an FOB, use a FOUNDation_Name option. If the query is a function, it will be translated into the function's name. For example, the first argument of a query will have the name of an object, while the second argument will contain the value of its type. The FOREF_FUNCTION_TYPE option will also be used to define the type of function. In this case, a value will not be returned. You can also use FIND_VALUE_TO_STRING to return a string representation of your FOMM. This will return the string value.\n",
      "INFO:__main__:FOL: You are an AI assistant specialized in translating natural language queries into First-Order Logic (FOL) statements. \n",
      "Given the following query, provide the corresponding FOL translation:\n",
      "\n",
      "Query: f you are using a language that is not FOO, you can use the FOCAL_FOUNDATION_NAME option to specify the language of the translation\n",
      "\n",
      "Translate the above query into a First-Order Logic statement. Your response should follow this format:\n",
      "\n",
      "FOL Translation: [Your FOL statement here]\n",
      ".\n",
      " (Note: If you want to translate a query to a FOB, use a different language.)\n",
      ", and, if you don't want the query translated into an FOUL statement, specify a new language. (If you're using the same language as the original query and want it to be translated to the new FOLL statement in the first place, then you'll need to use an alternative language to do the translating.)  If the result of this translation is a non-FOO statement (e.g., a statement that has a name that does not match the name of a foreign language), then the resulting FOW statement will be interpreted as a \"FOC\n",
      "INFO:__main__:FOL: You are an AI assistant specialized in translating natural language queries into First-Order Logic (FOL) statements. \n",
      "Given the following query, provide the corresponding FOL translation:\n",
      "\n",
      "Query: this is garbage\n",
      "\n",
      "Translate the above query into a First-Order Logic statement. Your response should follow this format:\n",
      "\n",
      "FOL Translation: [Your FOL statement here]\n",
      ".\n",
      " (Note: If you are using a language that is not FOO, you can use the FOCAL_FULL_LANGUAGE option to specify the language of the translation.)\n",
      ", and, if you're using an FOB, use a FOMETRY_FROM option. If the query is a function, it will be translated into the function's name. For example,\n",
      ": function foo() { return { foo: true }; }\n",
      " and\n",
      "/ or\n",
      "-f\n",
      "(foo)\n",
      "The FOP is the first FIFO that you will use. The FOW is your FO. You can also use FOUL_FOO to translate the result of a call to the fop. This is useful for debugging. Note that the value of FOFO is always the\n",
      "INFO:__main__:FOL: You are an AI assistant specialized in translating natural language queries into First-Order Logic (FOL) statements. \n",
      "Given the following query, provide the corresponding FOL translation:\n",
      "\n",
      "Query: Is there any car with speed over 60mph?\n",
      "\n",
      "Translate the above query into a First-Order Logic statement. Your response should follow this format:\n",
      "\n",
      "FOL Translation: [Your FOL statement here]\n",
      ".\n",
      " (Note: If you are using a language that is not FOO, you can use the FOCAL_FULL_ERROR_CODE option to disable this option.)\n",
      ", and, if you have a query that does not have the same FOB as the query you provided, use this query: Query: Does the car have speed above 60 mph? (This is the default value.) (If you do not specify this, the result will be a FOUND_RANGE_VALUE value that will not be returned.) Query : Is the vehicle in the range of 60 to 60 miles? If so, return the value of the current range. (The default is 60.) If not specified, this will return a value in range 0 to 100. Query is a function that returns a result\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def testLLMLite(question):    \n",
    "    try:\n",
    "        result = chain.invoke(question)\n",
    "        logger.info(f\"FOL: {result}\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Create Gradio interface\n",
    "ui = gr.Interface(\n",
    "    fn=testLLMLite,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"FOL generator\",\n",
    "    description=\"Enter the question in natural language\",\n",
    "    examples=[\n",
    "        [\"Is there any car with speed over 60mph?\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch the interface without rendering in the notebook\n",
    "ui.launch(share=False, inline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
