{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c6a145-ac9b-48cb-970b-f10fb6d2f96f",
   "metadata": {},
   "source": [
    "# üöÄ Project 1 -- Advanced LLM Integration with LangChain and Gradio\n",
    "\n",
    "Welcome to this advanced notebook on integrating Large Language Models (LLMs) with LangChain and Gradio! In this tutorial, we'll explore how to create a sophisticated chatbot that can use different tools to retreive information from external source and perform arithmatic operations.\n",
    "\n",
    "## üöÄ What we'll cover:\n",
    "\n",
    "1. Setting up a custom LLM\n",
    "2. Defining custom functions for arithmatic operations\n",
    "3. Implementing Wikipedia scraping and information extraction\n",
    "7. Using FAISS for vector storage and retrieval\n",
    "4. Binding custom functions with an LLM\n",
    "5. Writing a prompt to provide the LLM instructions for using custom functions\n",
    "6. Creating a conversation chain with memory\n",
    "8. Building a Gradio interface for user interaction\n",
    "\n",
    "Let's get started! üöÄ\n",
    "\n",
    "## 1Ô∏è‚É£ Setting up the Environment\n",
    "\n",
    "First, let's import the necessary libraries and set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14085c38-73ec-4b15-b905-9eeb56afe9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
    "from langchain.agents import AgentOutputParser  \n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "import requests\n",
    "import json\n",
    "from typing import Any, List, Mapping, Optional, Union\n",
    "import logging\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c6d6c-390a-41e6-b486-e9540153dbed",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Setting up the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa177889-281e-427f-8f1e-983dcc32c775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "class CustomLLM(LLM):\n",
    "    api_url: str = \"http://127.0.0.1:8899/v1/completions\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop = None,\n",
    "        run_manager = None,\n",
    "    ):\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        data = {\n",
    "            \"prompt\": prompt + \"\\nAnswer:\",\n",
    "            \"max_tokens\": 500,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 1.0,\n",
    "            \"n\": 1,\n",
    "            \"stop\": stop or [\"Human:\", \"\\n\\n\"]\n",
    "        }\n",
    "        try:\n",
    "            logger.info(f\"Sending prompt to API: {prompt}\")\n",
    "            response = requests.post(self.api_url, headers=headers, data=json.dumps(data))\n",
    "            response.raise_for_status()\n",
    "            result = response.json()['choices'][0]['text']\n",
    "            logger.info(f\"Received response from API: {result}\")\n",
    "            return result.strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"API request failed: {str(e)}\")\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"custom\"\n",
    "\n",
    "# Initialize the custom LLM\n",
    "llm = CustomLLM()\n",
    "\n",
    "# Initialize the conversation chain with a window memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "# change the device ID if needed\n",
    "embeddings = HuggingFaceEmbeddings(model_kwargs={\"device\": 1})\n",
    "\n",
    "# Initialize FAISS vector store\n",
    "vector_store = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf574e83-8ac2-446f-a4a5-e36715a88d1e",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Arithmetic operation functions for the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaaf866a-649c-4235-a87a-b63a54b2b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic operation functions\n",
    "\"\"\"\n",
    "These functions will take an input string representing two number separated by a comma. E.g., \"10, 12\".\n",
    "As such, we need to parse the input string to separate the numbers.\n",
    "\"\"\"\n",
    "\n",
    "def add(input_str: str):\n",
    "    numbers = input_str.split(',')\n",
    "    a = numbers[0].strip()\n",
    "    b = numbers[1].strip()\n",
    "    \n",
    "    \"\"\"Add two numbers. Notice that we put the main operation inside a try-catch block to handle malformed input. \n",
    "    This is a good practice whenever we implement a function for an LLM tool\n",
    "    \"\"\"\n",
    "    \n",
    "    try: \n",
    "        result = float(a) + float(b)\n",
    "        return f\"The result of {a} + {b} is {result}\"\n",
    "    except ValueError:\n",
    "        return \"Error: Please provide valid numbers for addition.\"\n",
    "\n",
    "def subtract(input_str: str):\n",
    "    numbers = input_str.split(',')\n",
    "    a = numbers[0].strip()\n",
    "    b = numbers[1].strip()\n",
    "    \n",
    "    try:\n",
    "        result = float(a) - float(b)\n",
    "        return f\"The result of {a} - {b} is {result}\"\n",
    "    except ValueError:\n",
    "        return \"Error: Please provide valid numbers for subtraction.\"\n",
    "\n",
    "def multiply(input_str: str):\n",
    "    numbers = input_str.split(',')\n",
    "    a = numbers[0].strip()\n",
    "    b = numbers[1].strip()\n",
    "    \n",
    "    try:\n",
    "        result = float(a) * float(b)\n",
    "        return f\"The result of {a} * {b} is {result}\"\n",
    "    except ValueError:\n",
    "        return \"Error: Please provide valid numbers for multiplication.\"\n",
    "\n",
    "def divide(input_str: str):\n",
    "    numbers = input_str.split(',')\n",
    "    a = numbers[0].strip()\n",
    "    b = numbers[1].strip()\n",
    "    \n",
    "    try:\n",
    "        a, b = float(a), float(b)\n",
    "        if b == 0:\n",
    "            return \"Error: Division by zero is not allowed.\"\n",
    "        result = a / b\n",
    "        return f\"The result of {a} / {b} is {result}\"\n",
    "    except ValueError:\n",
    "        return \"Error: Please provide valid numbers for division.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac222e-02e7-4a52-8340-56791f3595c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2856149d-0b54-4b24-a9c0-c512d9a4c458",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Implementing the Scraper and Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e10bcc-7705-4179-9b98-6599ce461196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_extract(content: str):\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following Wikipedia content in a few sentences:\n",
    "\n",
    "    {content[:1000]}  # Limited to 1000 characters\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm(prompt)\n",
    "    logger.info(f\"Extraction response: {response}\")\n",
    "    return response\n",
    "\n",
    "def scrape_wikipedia(url):\n",
    "    \"\"\"the input to this function will be an URL\"\"\"\n",
    "    \n",
    "    global vector_store\n",
    "    try:\n",
    "        # Validate URL\n",
    "        result = urlparse(url)\n",
    "        if not all([result.scheme, result.netloc]) or \"wikipedia.org\" not in result.netloc:\n",
    "            return \"Invalid URL. Please provide a complete Wikipedia URL.\"\n",
    "\n",
    "        # Load web content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the main content div\n",
    "        content_div = soup.find('div', {'id': 'mw-content-text'})\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "\n",
    "        # Extract content\n",
    "        content = []\n",
    "        if content_div:\n",
    "            for elem in content_div.find_all(['p', 'h2', 'h3']):\n",
    "                if elem.name == 'p':\n",
    "                    content.append(elem.text)\n",
    "                elif elem.name in ['h2', 'h3']:\n",
    "                    content.append(f\"\\n\\n{elem.text}\\n\")\n",
    "\n",
    "        full_content = f\"{title}\\n\\n{''.join(content)}\"\n",
    "        logger.info(f\"Scraped content (first 1000 chars): {full_content[:1000]}\")\n",
    "\n",
    "        # Extract content with simple function\n",
    "        extracted_content = simple_extract(full_content)\n",
    "        logger.info(f\"Extracted content: {extracted_content}\")\n",
    "\n",
    "        # Create or update the vector store\n",
    "        if vector_store is None:\n",
    "            vector_store = FAISS.from_texts([extracted_content], embeddings)\n",
    "        else:\n",
    "            vector_store.add_texts([extracted_content])\n",
    "        \n",
    "        return f\"Successfully scraped and extracted information from: {url}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping Wikipedia: {str(e)}\")\n",
    "        return f\"Error scraping Wikipedia: {str(e)}\"\n",
    "\n",
    "def query_vector_store(query):\n",
    "    \"\"\"the input to this function will be a user query\"\"\"\n",
    "    \n",
    "    if vector_store is None:\n",
    "        return \"No information has been scraped yet. Please provide a Wikipedia URL to scrape first.\"\n",
    "    \n",
    "    try:\n",
    "        docs = vector_store.similarity_search(query, k=1)\n",
    "        logger.info(f\"Retrieved {len(docs)} documents from vector store\")\n",
    "        for i, doc in enumerate(docs):\n",
    "            logger.info(f\"Document {i + 1} content: {doc.page_content[:100]}...\")  # Log first 100 chars of each document\n",
    "        \n",
    "        chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "        response = chain.run(input_documents=docs, question=query)\n",
    "        \n",
    "        logger.info(f\"Generated response: {response}\")\n",
    "        \n",
    "        if not response.strip():\n",
    "            return \"I apologize, but I couldn't generate a response based on the scraped information. Please try rephrasing your question.\"\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying vector store: {str(e)}\")\n",
    "        return f\"Error querying information: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3907725d-011f-47f0-8bb5-1b9bc30e116b",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Creating the tools with proper descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd283cc9-e0eb-4b1c-9511-eabc245646d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools; \n",
    "# Notice that tools is a list of Tool(...) object, which is defined by the LangChain framework \n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Addition\",\n",
    "        func=add, # this is the function we implemented before\n",
    "        description=\"Useful for adding two numbers together. Input should be two numbers separated by a comma.\" # we must provide this instruction to the LLM for choosing 1) the correct tool; 2) the correct input format\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Subtraction\",\n",
    "        func=subtract,\n",
    "        description=\"Useful for subtracting one number from another. Input should be two numbers separated by a comma.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Multiplication\",\n",
    "        func=multiply,\n",
    "        description=\"Useful for multiplying two numbers. Input should be two numbers separated by a comma.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Division\",\n",
    "        func=divide,\n",
    "        description=\"Useful for dividing one number by another. Input should be two numbers separated by a comma.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Wikipedia_Scraper\",\n",
    "        func=scrape_wikipedia,\n",
    "        description=\"Useful for scraping information from a Wikipedia page. Input should be a complete Wikipedia URL.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Information_Query\",\n",
    "        func=query_vector_store,\n",
    "        description=\"Useful for querying information from scraped Wikipedia pages. Input should be a question about the scraped content.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b7d69-ad47-4c5a-8128-5497d09554af",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£  Set up the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d9b4970-2237-4368-80f6-93d078e33176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the prompt template.\n",
    "# Notice that this is an extension of StringPromptTemplate defined by LangChain\n",
    "\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    \n",
    "    # declaring two variables\n",
    "    template: str\n",
    "    tools: List[Tool]\n",
    "    \n",
    "\n",
    "    # https://python.langchain.com/v0.1/docs/modules/agents/concepts/\n",
    "    def format(self, **kwargs):\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        \n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        \n",
    "        return self.template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0b595-d838-4968-a676-ef0312992fc5",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£  Writing a detailed prompt with all the instructions for the LLM\n",
    "- You can add more instructions according to your requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38304d60-7caa-4364-ac62-a8fab38b0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Important instructions:\n",
    "1. There might be multiple questions in the user prompt. Answer one by one if the answer requires the use of a tool.\n",
    "2. For arithmetic operations, ALWAYS use the corresponding tools (Addition, Subtraction, Multiplication, Division) directly. \n",
    "3. Use Wikipedia_Scraper only for scraping web pages. Use this tool when you are asked to scrape a website.\n",
    "4. Use Information_Query only for querying previously scraped information. When you are asked a question after scraping a website, use this tool to get context from scraped information.\n",
    "5. When you are asked to scrape a website, and there is no additional question in the user's prompt, scrape that website using the tool. In this case, your final answer should be: \"successfully scraped the website. You can ask questions regarding the website.\"\n",
    "6. Do not make up any questions and answers beyond what is asked.\n",
    "7. Do not repeat the question in your Final Answer.\n",
    "8. When you have finished answering ALL parts of the question, end your response with the marker [END_OF_RESPONSE].\n",
    "9. Do not add any text, questions, or conversation after the [END_OF_RESPONSE] marker.\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = CustomPromptTemplate(\n",
    "    template=prompt_template,\n",
    "    tools=tools,\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed2acb-7456-4de2-bbf5-aecd0fe21692",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Defining a custom output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca54b7bb-7cc7-4468-ae52-c4a236875201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str):\n",
    "        # Check for the end-of-response marker\n",
    "        if \"[END_OF_RESPONSE]\" in llm_output:\n",
    "            response = llm_output.split(\"[END_OF_RESPONSE]\")[0].strip()\n",
    "            \n",
    "            # Extract only the Final Answer\n",
    "            if \"Final Answer:\" in response:\n",
    "                final_answer = response.split(\"Final Answer:\")[-1].strip()\n",
    "                return AgentFinish(\n",
    "                    return_values={\"output\": final_answer},\n",
    "                    log=llm_output,\n",
    "                )\n",
    "            else:\n",
    "                return AgentFinish(\n",
    "                    return_values={\"output\": response},\n",
    "                    log=llm_output,\n",
    "                )\n",
    "\n",
    "        # Sanity Check: Check if this is the final answer\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            final_answer = llm_output.split(\"Final Answer:\")[-1].strip()\n",
    "            if \"[END_OF_RESPONSE]\" in final_answer:\n",
    "                final_answer = final_answer.split(\"[END_OF_RESPONSE]\")[0].strip()\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": final_answer},\n",
    "                log=llm_output,\n",
    "            )\n",
    "\n",
    "        # If it's not the final answer, parse the action\n",
    "        pattern = r\"Action: (.*?)\\nAction Input: (.*?)(?=\\n|$)\"\n",
    "        match = re.search(pattern, llm_output, re.DOTALL)\n",
    "\n",
    "        if not match:\n",
    "            # return llm_output\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": f\"I apologize, but I encountered an error. Please try again or rephrase your message.\"},\n",
    "                log=llm_output,\n",
    "            )\n",
    "            # raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "            \n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2).strip()\n",
    "    \n",
    "        logger.info(f'>>>> {action}: {action_input}')\n",
    "\n",
    "        return AgentAction(tool=action, tool_input=action_input, log=llm_output)\n",
    "\n",
    "        \n",
    "\n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730e8a8-4ce2-4f7c-9faf-445979d4e1d0",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Set up the agent that can use the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f8af267-a2a1-4e3a-b88d-8efe3f6de92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_186576/3895458415.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "/tmp/ipykernel_186576/3895458415.py:4: LangChainDeprecationWarning: Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.\n",
      "  agent = LLMSingleActionAgent(\n"
     ]
    }
   ],
   "source": [
    "# Set up the agent\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, \n",
    "    max_iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afce282-d1d6-4d8d-a6cb-fa2d65b9cadd",
   "metadata": {},
   "source": [
    "## üîü Creating the Chat Function\n",
    "\n",
    "Now, let's create the main chat function that will handle user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abadafc1-8c48-4671-930e-92aeb963caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    try:\n",
    "        response = agent_executor.run(message)\n",
    "        # Ensure we're only returning the final answer\n",
    "        if isinstance(response, dict) and \"output\" in response:\n",
    "            return response[\"output\"]\n",
    "        elif isinstance(response, str):\n",
    "            # If it's a string, it should already be the final answer, but let's make sure\n",
    "            if \"[END_OF_RESPONSE]\" in response:\n",
    "                response = response.split(\"[END_OF_RESPONSE]\")[0].strip()\n",
    "            return response\n",
    "        else:\n",
    "            return \"I apologize, but I couldn't generate a proper response. Please try again.\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during chat: {str(e)}\")\n",
    "        error_message = str(e)\n",
    "        if \"Could not parse LLM output\" in error_message:\n",
    "            return \"I'm sorry, I couldn't process that request correctly. Could you please rephrase your question?\"\n",
    "        return f\"I apologize, but I encountered an error. Please try again or rephrase your message.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20095cf3-869f-452f-8e3f-9286a848082c",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Setting up the Gradio Interface\n",
    "\n",
    "Finally, let's create a user-friendly interface using Gradio.\n",
    "\n",
    "Gradio will create a URL like http://127.0.0.1:7861 to access the interface. However, since the code is running on a remote server, this URL is not directly accessible from our local computer. To make it accessible, we need to enable port forwarding.\n",
    "\n",
    "*Follow these steps to access the interface from your web browser:*\n",
    "1. Go to the \"PORTS\" tab at the bottom of VS Code.\n",
    "2. Input the port number (in this case, 7863).\n",
    "3. Click on the browser icon. You will see the interface.\n",
    "\n",
    "*For those who prefer a command-line option:*\n",
    "1. Open a new terminal or command prompt window on your local computer.\n",
    "2. Enter the following command to forward the remote port to a local port:\n",
    "`ssh -L local_port:127.0.0.1:remote_port -J username@ssh.ist.psu.edu username@i4-cs-gpu01.ist.psu.edu` \n",
    "For example, if Gradio is running on 7863 port, my command looks this: `ssh -L 7861:localhost:7861 -J skb5969@ssh.ist.psu.edu skb5969@i4-cs-gpu01.ist.psu.edu`\n",
    "3. Open your browser, create a new tab, and enter http://127.0.0.1:port (in this case, http://127.0.0.1:7861). You will see the interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018c842d-242c-4ff8-8f39-8725de28f00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7878/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7878/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom CSS for full height\n",
    "custom_css = \"\"\"\n",
    "#chatbot-container {\n",
    "    height: calc(100vh - 230px) !important;\n",
    "    overflow-y: auto;\n",
    "}\n",
    "#input-container {\n",
    "    position: fixed;\n",
    "    bottom: 0;\n",
    "    left: 0;\n",
    "    right: 0;\n",
    "    padding: 20px;\n",
    "    background-color: white;\n",
    "    border-top: 1px solid #ccc;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(css=custom_css) as iface:\n",
    "    with gr.Column():\n",
    "        chatbot = gr.Chatbot(elem_id=\"chatbot-container\")\n",
    "        with gr.Row(elem_id=\"input-container\"):\n",
    "            msg = gr.Textbox(\n",
    "                show_label=False,\n",
    "                placeholder=\"Type your message here... (Use 'scrape:' for Wikipedia URLs or ask arithmetic questions)\",\n",
    "                container=False\n",
    "            )\n",
    "            send = gr.Button(\"Send\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "        bot_message = chat(user_message, history[:-1])\n",
    "        history[-1][1] = bot_message\n",
    "        return history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    send.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141132c5-6f5b-443b-9169-be87bbdef716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
